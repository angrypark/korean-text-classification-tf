{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.mode = 'train'\n",
    "        self.small = False\n",
    "        self.train_dir = \"data/train.txt\"\n",
    "        self.val_dir = \"data/test.txt\"\n",
    "        self.pretrained_embed_dir = \"\"\n",
    "        self.model = \"TextCNN\"\n",
    "        self.normalizer = \"BasicNormalizer\"\n",
    "        self.tokenizer = \"SyllableTokenizer\"\n",
    "        self.vectorizer = \"DummyVectorizer\"\n",
    "        self.vocab_size = 20000\n",
    "        self.embed_dim = 128\n",
    "        self.min_length = 5\n",
    "        self.max_length = 50\n",
    "        self.filter_sizes = \"3,4,5\"\n",
    "        self.num_filters = 128\n",
    "        self.dropout_keep_prob = 0.5\n",
    "        self.l2_reg_lambda = 0.0\n",
    "        self.batch_size = 64\n",
    "        self.num_epochs = 1\n",
    "        self.evaluate_every = 1\n",
    "        self.checkpoint_every = 1\n",
    "        self.allow_soft_replacement = True\n",
    "        self.log_device_placement = False\n",
    "        self.shuffle = False\n",
    "        self.checkpoint_dir = \"\"\n",
    "        self.num_checkpoints = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique tokens :  1662\n"
     ]
    }
   ],
   "source": [
    "data = DataGenerator(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for x_batch, y_batch in data.next_batch(32):\n",
    "    if i==0:\n",
    "        x = x_batch\n",
    "        y = y_batch\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 911,  406,  270, ...,    0,    0,    0],\n",
       "       [ 771,  950, 1163, ...,    0,    0,    0],\n",
       "       [ 652, 1036,    4, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 240,  347,    4, ...,    0,    0,    0],\n",
       "       [1300, 1300,    4, ...,    0,    0,    0],\n",
       "       [ 651, 1232,    4, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'180530_1809'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.strftime(\"%y%m%d_%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"int\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2b28a361b0ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"int\") to list"
     ]
    }
   ],
   "source": [
    "sum([1,2], [2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# weights initializers\n",
    "he_normal = tf.keras.initializers.he_normal()\n",
    "regularizer = tf.contrib.layers.l2_regularizer(1e-4)\n",
    "\n",
    "def Convolutional_Block(inputs, shortcut, num_filters, name, is_training):\n",
    "    print(\"-\"*20)\n",
    "    print(\"Convolutional Block\", str(num_filters), name)\n",
    "    print(\"-\"*20)\n",
    "    with tf.variable_scope(\"conv_block_\" + str(num_filters) + \"_\" + name):\n",
    "        for i in range(2):\n",
    "            with tf.variable_scope(\"conv1d_%s\" % str(i)):\n",
    "                filter_shape = [3, inputs.get_shape()[2], num_filters]\n",
    "                W = tf.get_variable(name='W', shape=filter_shape, \n",
    "                    initializer=he_normal,\n",
    "                    regularizer=regularizer)\n",
    "                inputs = tf.nn.conv1d(inputs, W, stride=1, padding=\"SAME\")\n",
    "                inputs = tf.layers.batch_normalization(inputs=inputs, momentum=0.997, epsilon=1e-5, \n",
    "                                                center=True, scale=True, training=is_training)\n",
    "                inputs = tf.nn.relu(inputs)\n",
    "                print(\"Conv1D:\", inputs.get_shape())\n",
    "    print(\"-\"*20)\n",
    "    if shortcut is not None:\n",
    "        print(\"-\"*5)\n",
    "        print(\"Optional Shortcut:\", shortcut.get_shape())\n",
    "        print(\"-\"*5)\n",
    "        return inputs + shortcut\n",
    "    return inputs\n",
    "\n",
    "# Three types of downsampling methods described by paper\n",
    "def downsampling(inputs, downsampling_type, name, optional_shortcut=False, shortcut=None):\n",
    "    # k-maxpooling\n",
    "    if downsampling_type=='k-maxpool':\n",
    "        k = math.ceil(int(inputs.get_shape()[1]) / 2)\n",
    "        pool = tf.nn.top_k(tf.transpose(inputs, [0,2,1]), k=k, name=name, sorted=False)[0]\n",
    "        pool = tf.transpose(pool, [0,2,1])\n",
    "    # Linear\n",
    "    elif downsampling_type=='linear':\n",
    "        pool = tf.layers.conv1d(inputs=inputs, filters=inputs.get_shape()[2], kernel_size=3,\n",
    "                            strides=2, padding='same', use_bias=False)\n",
    "    # Maxpooling\n",
    "    else:\n",
    "        pool = tf.layers.max_pooling1d(inputs=inputs, pool_size=3, strides=2, padding='same', name=name)\n",
    "    if optional_shortcut:\n",
    "        shortcut = tf.layers.conv1d(inputs=shortcut, filters=shortcut.get_shape()[2], kernel_size=1,\n",
    "                            strides=2, padding='same', use_bias=False)\n",
    "        print(\"-\"*5)\n",
    "        print(\"Optional Shortcut:\", shortcut.get_shape())\n",
    "        print(\"-\"*5)\n",
    "        pool += shortcut\n",
    "    pool = fixed_padding(inputs=pool)\n",
    "    return tf.layers.conv1d(inputs=pool, filters=pool.get_shape()[2]*2, kernel_size=1,\n",
    "                            strides=1, padding='valid', use_bias=False)\n",
    "\n",
    "def fixed_padding(inputs, kernel_size=3):\n",
    "    pad_total = kernel_size - 1\n",
    "    pad_beg = pad_total // 2\n",
    "    pad_end = pad_total - pad_beg\n",
    "    padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end], [0, 0]])\n",
    "    return padded_inputs\n",
    "\n",
    "class VDCNN():\n",
    "    def __init__(self, num_classes, sequence_max_length=1024, num_quantized_chars=69, embedding_size=16, \n",
    "                 depth=9, downsampling_type='maxpool', use_he_uniform=True, optional_shortcut=False):\n",
    "\n",
    "        # Depth to No. Layers\n",
    "        if depth == 9:\n",
    "            num_layers = [2,2,2,2]\n",
    "        elif depth == 17:\n",
    "            num_layers = [4,4,4,4]\n",
    "        elif depth == 29:\n",
    "            num_layers = [10,10,4,4]\n",
    "        elif depth == 49:\n",
    "            num_layers = [16,16,10,6]\n",
    "        else:\n",
    "            raise ValueError('depth=%g is a not a valid setting!' % depth)\n",
    "\n",
    "        # input tensors\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_max_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.is_training =  tf.placeholder(tf.bool)\n",
    "\n",
    "        # Embedding Lookup 16\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            if use_he_uniform:\n",
    "                self.embedding_W = tf.get_variable(name='lookup_W', shape=[num_quantized_chars, embedding_size], initializer=tf.keras.initializers.he_uniform())\n",
    "            else:\n",
    "                self.embedding_W = tf.Variable(tf.random_uniform([num_quantized_chars, embedding_size], -1.0, 1.0),name=\"embedding_W\")\n",
    "            self.embedded_characters = tf.nn.embedding_lookup(self.embedding_W, self.input_x)\n",
    "            print(\"-\"*20)\n",
    "            print(\"Embedded Lookup:\", self.embedded_characters.get_shape())\n",
    "            print(\"-\"*20)\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        # Temp(First) Conv Layer\n",
    "        with tf.variable_scope(\"temp_conv\") as scope: \n",
    "            filter_shape = [3, embedding_size, 64]\n",
    "            W = tf.get_variable(name='W_1', shape=filter_shape, \n",
    "                initializer=he_normal,\n",
    "                regularizer=regularizer)\n",
    "            inputs = tf.nn.conv1d(self.embedded_characters, W, stride=1, padding=\"SAME\")\n",
    "            #inputs = tf.nn.relu(inputs)\n",
    "        print(\"Temp Conv\", inputs.get_shape())\n",
    "        self.layers.append(inputs)\n",
    "\n",
    "        # Conv Block 64\n",
    "        for i in range(num_layers[0]):\n",
    "            if i < num_layers[0] - 1 and optional_shortcut:\n",
    "                shortcut = self.layers[-1]\n",
    "            else:\n",
    "                shortcut = None\n",
    "            conv_block = Convolutional_Block(inputs=self.layers[-1], shortcut=shortcut, num_filters=64, is_training=self.is_training, name=str(i+1))\n",
    "            self.layers.append(conv_block)\n",
    "        pool1 = downsampling(self.layers[-1], downsampling_type=downsampling_type, name='pool1', optional_shortcut=optional_shortcut, shortcut=self.layers[-2])\n",
    "        self.layers.append(pool1)\n",
    "        print(\"Pooling:\", pool1.get_shape())\n",
    "\n",
    "        # Conv Block 128\n",
    "        for i in range(num_layers[1]):\n",
    "            if i < num_layers[1] - 1 and optional_shortcut:\n",
    "                shortcut = self.layers[-1]\n",
    "            else:\n",
    "                shortcut = None\n",
    "            conv_block = Convolutional_Block(inputs=self.layers[-1], shortcut=shortcut, num_filters=128, is_training=self.is_training, name=str(i+1))\n",
    "            self.layers.append(conv_block)\n",
    "        pool2 = downsampling(self.layers[-1], downsampling_type=downsampling_type, name='pool2', optional_shortcut=optional_shortcut, shortcut=self.layers[-2])\n",
    "        self.layers.append(pool2)\n",
    "        print(\"Pooling:\", pool2.get_shape())\n",
    "\n",
    "        # Conv Block 256\n",
    "        for i in range(num_layers[2]):\n",
    "            if i < num_layers[2] - 1 and optional_shortcut:\n",
    "                shortcut = self.layers[-1]\n",
    "            else:\n",
    "                shortcut = None\n",
    "            conv_block = Convolutional_Block(inputs=self.layers[-1], shortcut=shortcut, num_filters=256, is_training=self.is_training, name=str(i+1))\n",
    "            self.layers.append(conv_block)\n",
    "        pool3 = downsampling(self.layers[-1], downsampling_type=downsampling_type, name='pool3', optional_shortcut=optional_shortcut, shortcut=self.layers[-2])\n",
    "        self.layers.append(pool3)\n",
    "        print(\"Pooling:\", pool3.get_shape())\n",
    "\n",
    "        # Conv Block 512\n",
    "        for i in range(num_layers[3]):\n",
    "            if i < num_layers[3] - 1 and optional_shortcut:\n",
    "                shortcut = self.layers[-1]\n",
    "            else:\n",
    "                shortcut = None\n",
    "            conv_block = Convolutional_Block(inputs=self.layers[-1], shortcut=shortcut, num_filters=512, is_training=self.is_training, name=str(i+1))\n",
    "            self.layers.append(conv_block)\n",
    "\n",
    "        # Extract 8 most features as mentioned in paper\n",
    "        self.k_pooled = tf.nn.top_k(tf.transpose(self.layers[-1], [0,2,1]), k=8, name='k_pool', sorted=False)[0]\n",
    "        print(\"8-maxpooling:\", self.k_pooled.get_shape())\n",
    "        self.flatten = tf.reshape(self.k_pooled, (-1, 512*8))\n",
    "\n",
    "        # fc1\n",
    "        with tf.variable_scope('fc1'):\n",
    "            w = tf.get_variable('w', [self.flatten.get_shape()[1], 2048], initializer=he_normal,\n",
    "                regularizer=regularizer)\n",
    "            b = tf.get_variable('b', [2048], initializer=tf.constant_initializer(1.0))\n",
    "            out = tf.matmul(self.flatten, w) + b\n",
    "            self.fc1 = tf.nn.relu(out)\n",
    "\n",
    "        # fc2\n",
    "        with tf.variable_scope('fc2'):\n",
    "            w = tf.get_variable('w', [self.fc1.get_shape()[1], 2048], initializer=he_normal,\n",
    "                regularizer=regularizer)\n",
    "            b = tf.get_variable('b', [2048], initializer=tf.constant_initializer(1.0))\n",
    "            out = tf.matmul(self.fc1, w) + b\n",
    "            self.fc2 = tf.nn.relu(out)\n",
    "\n",
    "        # fc3\n",
    "        with tf.variable_scope('fc3'):\n",
    "            w = tf.get_variable('w', [self.fc2.get_shape()[1], num_classes], initializer=he_normal,\n",
    "                regularizer=regularizer)\n",
    "            b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(1.0))\n",
    "            self.fc3 = tf.matmul(self.fc2, w) + b\n",
    "\n",
    "        # Calculate Mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.predictions = tf.argmax(self.fc3, 1, name=\"predictions\")\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.fc3, labels=self.input_y)\n",
    "            regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "            self.loss = tf.reduce_mean(losses) + sum(regularization_losses)\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
